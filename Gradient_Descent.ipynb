{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/James-Zhang-23/FarmSense/blob/main/Gradient_Descent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PoHxEsWmqO_V"
      },
      "source": [
        "# EE 599 - HW 1 - Part 1 Gradient Descent\n",
        "\n",
        "Your task in this Colab notebook is to fill out the sections that are specified by **TODO**.\n",
        "\n",
        "# Resources\n",
        "* [Get Started with Google Colaboratory](https://youtu.be/inN8seMm7UI)\n",
        "* [Google Colab: Basic Features Overview](https://colab.research.google.com/notebooks/basic_features_overview.ipynb)\n",
        "* [Uploading and Saving GitHub Notebooks](https://colab.research.google.com/github/googlecolab/colabtools/blob/master/notebooks/colab-github-demo.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_XuWmiBbY9Z2"
      },
      "outputs": [],
      "source": [
        "# The following code should not be modified.\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pprint\n",
        "import sys\n",
        "\n",
        "\n",
        "# Create a PrettyPrinter object with an indentation of 4 spaces\n",
        "pp = pprint.PrettyPrinter(indent=4)\n",
        "\n",
        "%matplotlib inline\n",
        "plt.style.use(['ggplot'])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x-bolx8sY3Na"
      },
      "outputs": [],
      "source": [
        "# The following code should not be modified. You can use the dbg function for debugging your variables.\n",
        "# The following code mounts the utility folder to your working directry\n",
        "# Then, you can import modules inside UtilFolder in your google drive.\n",
        "# This code block will prompt you to login to you Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "sys.path.append('/content/drive/MyDrive/UtilFolder/')\n",
        "from utils import dbg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FSaMV4aUiE-r"
      },
      "source": [
        "# Gradient Descent for Linear Regression\n",
        "\n",
        "Given two vectors, X and Y, that represent the input and output values of a linear relationship, we can fit a line to this data to predict the output for other values of X. This can be done by finding the parameters $\\theta_0$ and $\\theta_1$ of the line, such that the line minimizes the squared error between the predicted and actual output values.\n",
        "\n",
        "The equation for the line is:\n",
        "\n",
        "\\begin{equation}\n",
        "\\hat{y} =\\theta_0 + \\theta_1 \\cdot x\n",
        "\\end{equation}\n",
        "\n",
        "where:\n",
        "\n",
        "* $\\hat{y}$ is the predicted output value.\n",
        "* $\\theta_0$ is the intercept\n",
        "* $\\theta_1$ is the slope\n",
        "\n",
        "Let's use Gradient Descent to find the best values for $\\mathbf{\\Theta}$ as defined below:\n",
        "\n",
        "\\begin{equation}\n",
        "\\mathbf{\\Theta} = \\begin{bmatrix} \\theta_0 \\\\ \\theta_1 \\end{bmatrix} \\end{equation}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YjrBt3VLY9aD"
      },
      "source": [
        "# Create Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3YaEIonqY9aF"
      },
      "source": [
        "\n",
        "<h5> Let's generate some data with:\n",
        "\n",
        "\\begin{equation} \\mathbf{\\Theta} = \\begin{bmatrix} 3 \\\\ 2 \\end{bmatrix} \\end{equation}\n",
        "\n",
        "We will use this data to fit the line and then use it to predict the output for other values of $x$.\n",
        "\n",
        "**TODO:** Below, write the Python code such that:\n",
        "*   `number_of_samples` is an integer variable\n",
        "*   X is a vector of `number_of_samples` numbers between 1, 100\n",
        "*   Y is a vector of `number_of_samples` numbers where each $y = \\theta_0 + \\theta_1 \\cdot x$ plus some noise, which is a random value between 0 and 1.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6uZUKbTJY9aG"
      },
      "outputs": [],
      "source": [
        "number_of_samples = 100\n",
        "\n",
        "X = 2 * np.random.rand(number_of_samples, 1)\n",
        "y = 3 + 2 * X +  np.random.rand(number_of_samples, 1)\n",
        "\n",
        "\n",
        "theta_guess = np.array([[1.0], [1.0]])\n",
        "learning_rate = 0.001\n",
        "num_iterations = 500\n",
        "\n",
        "final_cost = {}\n",
        "final_theta = {}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ySi6YTFiY9aL"
      },
      "source": [
        "**TODO:** Plot and visualize your data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hGwW_tP3Y9aM"
      },
      "outputs": [],
      "source": [
        "plt.plot(X,y,'b.')\n",
        "plt.xlabel(\"$x$\", fontsize=18)\n",
        "plt.ylabel(\"$y$\", rotation=0, fontsize=18)\n",
        "_ =plt.axis([0,np.max(X),0,np.max(y)])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ORr_MxoXY9al"
      },
      "source": [
        "# Gradient Descent Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yOs3ormtY9al"
      },
      "source": [
        "## Cost Function & Gradients\n",
        "\n",
        "<h4> The equation for calculating cost function and gradients are as shown below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0F0hkmH2Y9am"
      },
      "source": [
        "### Cost:\n",
        "\\begin{equation}\n",
        "J(\\theta_0, \\theta_1) = \\text{MSE}(\\theta_0, \\theta_1) = \\frac{1}{n} \\sum_{i=1}^n (\\hat{y}_i - y_i)^2\n",
        "\\end{equation}\n",
        "\n",
        "### Gradient:\n",
        "\n",
        "\\begin{equation}\n",
        "\\nabla J(\\theta_0, \\theta_1) = \\left( \\frac{\\partial J}{\\partial \\theta_0}, \\frac{\\partial J}{\\partial \\theta_1} \\right)\n",
        "\\end{equation}\n",
        "\n",
        "### Replacing $MSE$ for $J$:\n",
        "\n",
        "\\begin{equation}\n",
        "\\nabla J(\\theta_0, \\theta_1) = \\left( \\frac{2}{n} \\sum_{i=1}^n (\\hat{y}_i - y_i) , \\frac{2}{n} \\sum_{i=1}^n (\\hat{y}_i - y_i) \\cdot \\mathbf{\\hat{X}}_i \\right)\n",
        "\\end{equation}\n",
        "\n",
        "**TODO:** Explain how did we come up with each equation?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4RDfLeM49EjM"
      },
      "source": [
        "**TODO:** In the next cell, Write the Python code for the following functions:\n",
        "\n",
        "\n",
        "```python\n",
        "def calculate_prediction_residuals(theta, X, y):\n",
        "    \"\"\"\n",
        "    Calculate the predictions and residuals for a linear regression model.\n",
        "\n",
        "   Parameters:\n",
        "    - theta (numpy.ndarray): Parameter vector of shape (num_features,)\n",
        "    - X (numpy.ndarray): Feature matrix of shape (num_samples, num_features)\n",
        "     Each row represents a sample, and each column represents a feature.\n",
        "                         The first column should be filled with ones for the intercept term.\n",
        "                         Example:\n",
        "                         [[1, feature_1_sample_1, feature_2_sample_1],\n",
        "                          [1, feature_1_sample_2, feature_2_sample_2],\n",
        "                          ...\n",
        "                          [1, feature_1_sample_n, feature_2_sample_n]]\n",
        "    - y (numpy.ndarray): Target values of shape (num_samples,)\n",
        "\n",
        "\n",
        "    Returns:\n",
        "    - predictions (numpy.ndarray): Model predictions of shape (num_samples,).\n",
        "    - residuals (numpy.ndarray): Model residuals of shape (num_samples,).\n",
        "    \"\"\"\n",
        "\n",
        "def calculate_cost(theta, X, y):\n",
        "    \"\"\"\n",
        "    Calculate the cost (Mean Squared Error) for a linear regression model.\n",
        "\n",
        "    Parameters:\n",
        "    - theta (numpy.ndarray): Parameter vector of shape (num_features,)\n",
        "    - X (numpy.ndarray): Feature matrix of shape (num_samples, num_features)\n",
        "     Each row represents a sample, and each column represents a feature.\n",
        "                         The first column should be filled with ones for the intercept term.\n",
        "                         Example:\n",
        "                         [[1, feature_1_sample_1, feature_2_sample_1],\n",
        "                          [1, feature_1_sample_2, feature_2_sample_2],\n",
        "                          ...\n",
        "                          [1, feature_1_sample_n, feature_2_sample_n]]\n",
        "    - y (numpy.ndarray): Target values of shape (num_samples,)\n",
        "\n",
        "    Returns:\n",
        "    - cost (float): Computed cost value\n",
        "    \"\"\"\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KdAHnLQKY9ao"
      },
      "outputs": [],
      "source": [
        "def calculate_prediction_residuals(theta, X, y):\n",
        "    \"\"\"\n",
        "    Calculate the predictions and residuals for a linear regression model.\n",
        "\n",
        "    Parameters:\n",
        "    - theta (numpy.ndarray): Parameter vector of shape (num_features,).\n",
        "    - X (numpy.ndarray): Feature matrix of shape (num_samples, num_features)\n",
        "     Each row represents a sample, and each column represents a feature.\n",
        "                         The first column should be filled with ones for the intercept term.\n",
        "                         Example:\n",
        "                         [[1, feature_1_sample_1, feature_2_sample_1],\n",
        "                          [1, feature_1_sample_2, feature_2_sample_2],\n",
        "                          ...\n",
        "                          [1, feature_1_sample_n, feature_2_sample_n]]\n",
        "    - y (numpy.ndarray): Target values of shape (num_samples,).\n",
        "\n",
        "    Returns:\n",
        "    - predictions (numpy.ndarray): Model predictions of shape (num_samples,).\n",
        "    - residuals (numpy.ndarray): Model residuals of shape (num_samples,).\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "\n",
        "    return None\n",
        "\n",
        "\n",
        "def calculate_cost(theta, X, y):\n",
        "    \"\"\"\n",
        "    Calculate the cost (Mean Squared Error) for a linear regression model.\n",
        "\n",
        "    Parameters:\n",
        "    - theta (numpy.ndarray): Parameter vector of shape (num_features,)\n",
        "    - X (numpy.ndarray): Feature matrix of shape (num_samples, num_features)\n",
        "     Each row represents a sample, and each column represents a feature.\n",
        "                         The first column should be filled with ones for the intercept term.\n",
        "                         Example:\n",
        "                         [[1, feature_1_sample_1, feature_2_sample_1],\n",
        "                          [1, feature_1_sample_2, feature_2_sample_2],\n",
        "                          ...\n",
        "                          [1, feature_1_sample_n, feature_2_sample_n]]\n",
        "    - y (numpy.ndarray): Target values of shape (num_samples,)\n",
        "\n",
        "    Returns:\n",
        "    - cost (float): Computed cost value\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    return None\n",
        "\n",
        "def calculate_gradient(X, residuals):\n",
        "    \"\"\"\n",
        "    Calculate the gradient for linear regression.\n",
        "\n",
        "    Parameters:\n",
        "     - X (numpy.ndarray): Feature matrix of shape (num_samples, num_features)\n",
        "     Each row represents a sample, and each column represents a feature.\n",
        "                         The first column should be filled with ones for the intercept term.\n",
        "                         Example:\n",
        "                         [[1, feature_1_sample_1, feature_2_sample_1],\n",
        "                          [1, feature_1_sample_2, feature_2_sample_2],\n",
        "                          ...\n",
        "                          [1, feature_1_sample_n, feature_2_sample_n]]\n",
        "    - residuals (numpy.ndarray): Residuals (difference between predictions and target values) of shape (num_samples,).\n",
        "\n",
        "    Returns:\n",
        "    - gradient (numpy.ndarray): Gradient vector of shape (num_features,).\n",
        "    \"\"\"\n",
        "\n",
        "    return None\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZ8hQ0MB9rpR"
      },
      "source": [
        "**TODO:** In the next cell, Write the Python code to implement Gradient Descent. The function signature should be:\n",
        "\n",
        "\n",
        "\n",
        "```python\n",
        "gradient_descent(X, y, theta, learning_rate, num_iterations):\n",
        "    \"\"\"\n",
        "    Perform gradient descent to optimize parameters for linear regression.\n",
        "\n",
        "    Parameters:\n",
        "    - X (numpy.ndarray): Feature matrix of shape (num_samples, num_features).\n",
        "                         Each row represents a sample, and each column represents a feature.\n",
        "                         The first column should be filled with ones for the intercept term.\n",
        "                         Example:\n",
        "                         [[1, feature_1_sample_1, feature_2_sample_1],\n",
        "                          [1, feature_1_sample_2, feature_2_sample_2],\n",
        "                          ...\n",
        "                          [1, feature_1_sample_n, feature_2_sample_n]]\n",
        "    - y (numpy.ndarray): Target values of shape (num_samples,).\n",
        "    - theta (numpy.ndarray): Initial parameter vector of shape (num_features,).\n",
        "    - learning_rate (float): Learning rate for gradient descent.\n",
        "    - num_iterations (int): Number of iterations for gradient descent.\n",
        "\n",
        "    Returns:\n",
        "    - theta (numpy.ndarray): Optimized parameter vector.\n",
        "    - cost_history (list): List of cost values over iterations.\n",
        "    - theta_history (list): List of parameter vectors over iterations.\n",
        "    \"\"\"\n",
        "```\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KdrOopKWBonL"
      },
      "outputs": [],
      "source": [
        "def gradient_descent(X, y, theta, learning_rate, num_iterations):\n",
        "    \"\"\"\n",
        "    Perform gradient descent to optimize parameters for linear regression.\n",
        "\n",
        "    Parameters:\n",
        "    - X (numpy.ndarray): Feature matrix of shape (num_samples, num_features).\n",
        "                         Each row represents a sample, and each column represents a feature.\n",
        "                         The first column should be filled with ones for the intercept term.\n",
        "                         Example:\n",
        "                         [[1, feature_1_sample_1, feature_2_sample_1],\n",
        "                          [1, feature_1_sample_2, feature_2_sample_2],\n",
        "                          ...\n",
        "                          [1, feature_1_sample_n, feature_2_sample_n]]\n",
        "    - y (numpy.ndarray): Target values of shape (num_samples,).\n",
        "    - theta (numpy.ndarray): Initial parameter vector of shape (num_features,).\n",
        "    - learning_rate (float): Learning rate for gradient descent.\n",
        "    - num_iterations (int): Number of iterations for gradient descent.\n",
        "\n",
        "    Returns:\n",
        "    - theta (numpy.ndarray): Optimized parameter vector.\n",
        "    - cost_history (list): List of cost values over iterations.\n",
        "    - theta_history (list): List of parameter vectors over iterations.\n",
        "    \"\"\"\n",
        "    return None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2hZ9kXyUY9ax"
      },
      "source": [
        "**TODO** In the next cell, run the GD implementation, store the cost for each iteration and plot the cost vs. iterations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rh60XqBnY9ax"
      },
      "outputs": [],
      "source": [
        "theta = theta_guess.copy()\n",
        "\n",
        "print(\"Start:\")\n",
        "print(\n",
        "    \"Theta0:          {:0.3f},\\nTheta1:          {:0.3f}\".format(\n",
        "        theta[0][0], theta[1][0]\n",
        "    )\n",
        ")\n",
        "\n",
        "# Add a column of ones to the feature matrix X:\n",
        "X_b = np.c_[np.ones((len(X), 1)), X]\n",
        "\n",
        "# Run Gradient Descent:\n",
        "theta, cost_history, theta_history = gradient_descent(\n",
        "    X_b, y, theta, learning_rate, num_iterations\n",
        ")\n",
        "final_theta['GD']= theta\n",
        "\n",
        "\n",
        "print(\"End:\")\n",
        "print(\n",
        "    \"Theta0:          {:0.3f},\\nTheta1:          {:0.3f}\".format(\n",
        "        theta[0][0], theta[1][0]\n",
        "    )\n",
        ")\n",
        "print(\"Final cost/MSE:  {:0.3f}\".format(cost_history[-1]))\n",
        "final_cost['GD']= cost_history[-1]\n",
        "\n",
        "fig,ax = plt.subplots(figsize=(10,8))\n",
        "\n",
        "ax.set_ylabel('J(Theta)')\n",
        "ax.set_xlabel('Iterations')\n",
        "_=ax.plot(range(num_iterations),cost_history,'b.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j0CVFwvtY9bW"
      },
      "source": [
        "### (Optional) Build a function which can show the effects together and also show how gradient decent actually is working"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u71DHEAeY9bZ"
      },
      "outputs": [],
      "source": [
        "def plot_gradient_descent(\n",
        "    num_iterations, learning_rate, X_b, y, data_axis, cost_axis=None\n",
        "):\n",
        "    \"\"\"\n",
        "    Visualize gradient descent optimization for linear regression.\n",
        "\n",
        "    Parameters:\n",
        "    - num_iterations (int): Number of iterations for gradient descent.\n",
        "    - learning_rate (float): Learning rate for gradient descent.\n",
        "    - X_b (numpy.ndarray): Feature matrix of shape (num_samples, num_features).\n",
        "    - y (numpy.ndarray): Target values of shape (num_samples,).\n",
        "    - data_axis (matplotlib Axis): Axis to visualize the data points and regression lines.\n",
        "    - cost_axis (matplotlib Axis): Optional axis to visualize the cost history vs. iterations.\n",
        "\n",
        "    \"\"\"\n",
        "    # Plot the data points as blue dots\n",
        "    data_axis.plot(\n",
        "        # Assuming the first column is for the intercept term, select the\n",
        "        # second column.\n",
        "        X_b[:, 1], y, \"b.\"\n",
        "    )\n",
        "    # Initialize random theta values from a standard normal distribution\n",
        "    theta = np.random.randn(2, 1)\n",
        "\n",
        "    # Initial transparency value for plotting regression lines\n",
        "    transparency = 0.05\n",
        "\n",
        "    # Initialize an array to store cost history during iterations\n",
        "    cost_history = np.zeros(num_iterations)\n",
        "\n",
        "    # Loop through each iteration\n",
        "    for i in range(num_iterations):\n",
        "        # Compute predictions using the current theta values\n",
        "        predictions_prev = X_b.dot(theta)\n",
        "\n",
        "        # Perform one iteration of gradient descent to update theta values\n",
        "        theta, current_cost, _ = gradient_descent(X_b, y, theta, learning_rate, 1)\n",
        "\n",
        "        # Compute predictions using the updated theta values\n",
        "        predictions = X_b.dot(theta)\n",
        "\n",
        "        # Store the cost value after the current iteration\n",
        "        cost_history[i] = current_cost[0]\n",
        "\n",
        "        # Plot the regression line with adjusted transparency\n",
        "        if i % 25 == 0:\n",
        "            data_axis.plot(\n",
        "                X_b[:, 1], predictions, \"r-\", alpha=transparency\n",
        "            )  # Assuming the first column is for the intercept term\n",
        "            if transparency < 0.8:\n",
        "                transparency += 0.15\n",
        "\n",
        "    # If provided, plot cost history vs. iterations\n",
        "    if cost_axis is not None:\n",
        "        cost_axis.plot(range(num_iterations), cost_history, \"b.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tuI2x9deY9br"
      },
      "source": [
        "### (Optional) Plot the graphs for different iterations and learning rates combination"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uJNi998qY9bv"
      },
      "outputs": [],
      "source": [
        "# Create a figure with subplots for visualization\n",
        "fig = plt.figure(figsize=(20, 20))\n",
        "fig.subplots_adjust(hspace=0.2, wspace=0.2)\n",
        "\n",
        "# List of iteration and learning rate pairs to iterate through\n",
        "number_of_iteration_learning_rate_pairs = [(1000, 0.001), (500, 0.01), (100, 0.02)]\n",
        "\n",
        "# Initialize the subplot count for positioning\n",
        "subplot_count = 0\n",
        "\n",
        "# Loop through each iteration and learning rate pair\n",
        "for number_of_iterations, learning_rate in number_of_iteration_learning_rate_pairs:\n",
        "    # Increment the subplot count and create data and cost axes\n",
        "    subplot_count += 1\n",
        "    data_axis = fig.add_subplot(4, 2, subplot_count)\n",
        "    subplot_count += 1\n",
        "    cost_axis = fig.add_subplot(4, 2, subplot_count)\n",
        "\n",
        "    # Set titles for the data and cost axes\n",
        "    data_axis.set_title(\"Learning Rate: {}\".format(learning_rate))\n",
        "    cost_axis.set_title(\"Iterations: {}\".format(number_of_iterations))\n",
        "\n",
        "    # Call the function to visualize gradient descent\n",
        "    plot_gradient_descent(\n",
        "        number_of_iterations, learning_rate, X_b, y, data_axis, cost_axis\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UBx-2txOY9cM"
      },
      "source": [
        "# Stochastic Gradient Descent\n",
        "\n",
        "The main difference between SGD and GD is that SGD uses a **single training example** to update the model parameters at each iteration, while GD uses all of the training examples.\n",
        "\n",
        "\n",
        "* SGD is less computationally expensive than GD and can converge faster. However, SGD can be more noisy than GD, which means that it may not always converge to the global minimum of the function.\n",
        "* GD is more robust to outliers and is less likely to get stuck in local minima. However, GD can be more computationally expensive than SGD and may take longer to converge.\n",
        "\n",
        "\n",
        "**TODO** In the next cell a function to implement SGD. You can use this function signature:\n",
        "\n",
        "```python\n",
        "stochastic_gradient_descent(X, y, theta, learning_rate=0.01, iterations=10):\n",
        "    \"\"\"\n",
        "    Perform stochastic gradient descent to optimize parameters for linear regression.\n",
        "\n",
        "    Parameters:\n",
        "    - X (numpy.ndarray): Feature matrix of shape (num_samples, num_features).\n",
        "                         Each row represents a sample, and each column represents a feature.\n",
        "                         The first column should be filled with ones for the intercept term.\n",
        "                         Example:\n",
        "                         [[1, feature_1_sample_1, feature_2_sample_1],\n",
        "                          [1, feature_1_sample_2, feature_2_sample_2],\n",
        "                          ...\n",
        "                          [1, feature_1_sample_n, feature_2_sample_n]]\n",
        "    - y (numpy.ndarray): Target values of shape (num_samples,).\n",
        "    - theta (numpy.ndarray): Initial parameter vector.\n",
        "    - learning_rate (float): Learning rate for stochastic gradient descent.\n",
        "    - iterations (int): Number of iterations for stochastic gradient descent.\n",
        "\n",
        "    Returns:\n",
        "    - theta (numpy.ndarray): Optimized parameter vector.\n",
        "    - cost_history (numpy.ndarray): Array of cost values over iterations.\n",
        "    \"\"\"\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s6VcRoNfY9ck"
      },
      "outputs": [],
      "source": [
        "def stochastic_gradient_descent(X, y, theta, learning_rate=0.01, iterations=10):\n",
        "    \"\"\"\n",
        "    Perform stochastic gradient descent to optimize parameters for linear regression.\n",
        "\n",
        "    Parameters:\n",
        "    - X (numpy.ndarray): Feature matrix of shape (num_samples, num_features).\n",
        "                         Each row represents a sample, and each column represents a feature.\n",
        "                         The first column should be filled with ones for the intercept term.\n",
        "                         Example:\n",
        "                         [[1, feature_1_sample_1, feature_2_sample_1],\n",
        "                          [1, feature_1_sample_2, feature_2_sample_2],\n",
        "                          ...\n",
        "                          [1, feature_1_sample_n, feature_2_sample_n]]\n",
        "    - y (numpy.ndarray): Target values of shape (num_samples,).\n",
        "    - theta (numpy.ndarray): Initial parameter vector.\n",
        "    - learning_rate (float): Learning rate for stochastic gradient descent.\n",
        "    - iterations (int): Number of iterations for stochastic gradient descent.\n",
        "\n",
        "    Returns:\n",
        "    - theta (numpy.ndarray): Optimized parameter vector.\n",
        "    - cost_history (numpy.ndarray): Array of cost values over iterations.\n",
        "    \"\"\"\n",
        "    return None\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M14WZu56v1vR"
      },
      "source": [
        "**TODO**: Run your SGD implementation and plot the cost vs iteration."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oOyOz_Vh321t"
      },
      "source": [
        "# Stochastic Gradient Descent with Momentum\n",
        "\n",
        "Stochastic Gradient Descent (SGD) with momentum is an optimization technique that incorporates momentum to enhance the convergence of the optimization process. It is particularly useful for training machine learning models, including neural networks. The momentum term helps to smooth out variations in the gradient updates and accelerates convergence along the steepest direction.\n",
        "\n",
        "In SGD with momentum, the update rule for the parameters involves a velocity term that accumulates a fraction of the previous gradients. This velocity term adds inertia to the updates, helping the optimization process to move more consistently and smoothly through the optimization landscape. The formula for the update step is as follows:\n",
        "\n",
        "## Velocity Update:\n",
        "\n",
        "\\begin{equation}\n",
        "v_t = \\beta \\cdot v_{t-1} + (1 - \\beta) \\cdot \\nabla J(\\theta_t)\n",
        "\\end{equation}\n",
        "\n",
        "\n",
        "## Parameter Update:\n",
        "\n",
        "\\begin{equation}\n",
        "\\theta_{t+1} = \\theta_t - \\alpha \\cdot v_t\n",
        "\\end{equation}\n",
        "\n",
        "* $v_t$ is the velocity at iteration $t$.\n",
        "* $\\beta$ is the momentum parameter between 0 and 1, controlling the retention of previous velocity.\n",
        "* $\\nabla J(\\theta_{t})$ is the gradient of the cost function at iteration $t$ with respect to the parameters $\\theta_{t}$\n",
        "* $\\alpha$ is the learning rate.\n",
        "\n",
        "The velocity term $v_t$ accumulates a fraction of the previous velocity $v_{t-1}$ and adds the current gradient update $\\nabla J(\\theta)$\n",
        " scaled by $(1-\\beta)$. The parameters $\\theta_{t+1}$ are then updated by subtracting the scaled velocity term.\n",
        "\n",
        "**TODO**: Implement the SGD with Momentum. You can use this signature:\n",
        "\n",
        "```python\n",
        "def stochastic_gradient_descent_with_momentum(X, y, theta, learning_rate=0.01, momentum=0.9, iterations=10):\n",
        "    \"\"\"\n",
        "    Perform stochastic gradient descent with momentum to optimize parameters for linear regression.\n",
        "\n",
        "    Parameters:\n",
        "    - X (numpy.ndarray): Feature matrix of shape (num_samples, num_features).\n",
        "                         Each row represents a sample, and each column represents a feature.\n",
        "                         The first column should be filled with ones for the intercept term.\n",
        "                         Example:\n",
        "                         [[1, feature_1_sample_1, feature_2_sample_1],\n",
        "                          [1, feature_1_sample_2, feature_2_sample_2],\n",
        "                          ...\n",
        "                          [1, feature_1_sample_n, feature_2_sample_n]]\n",
        "    - y (numpy.ndarray): Target values of shape (num_samples,).\n",
        "    - theta (numpy.ndarray): Initial parameter vector.\n",
        "    - learning_rate (float): Learning rate for stochastic gradient descent.\n",
        "    - momentum (float): Momentum coefficient.\n",
        "    - iterations (int): Number of iterations for stochastic gradient descent.\n",
        "\n",
        "    Returns:\n",
        "    - theta (numpy.ndarray): Optimized parameter vector.\n",
        "    - cost_history (numpy.ndarray): Array of cost values over iterations.\n",
        "    \"\"\"\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q20nfvlJ38js"
      },
      "outputs": [],
      "source": [
        "def stochastic_gradient_descent_with_momentum(X, y, theta, learning_rate=0.01, momentum=0.9, iterations=10):\n",
        "    \"\"\"\n",
        "    Perform stochastic gradient descent with momentum to optimize parameters for linear regression.\n",
        "\n",
        "    Parameters:\n",
        "    - X (numpy.ndarray): Feature matrix of shape (num_samples, num_features).\n",
        "                         Each row represents a sample, and each column represents a feature.\n",
        "                         The first column should be filled with ones for the intercept term.\n",
        "                         Example:\n",
        "                         [[1, feature_1_sample_1, feature_2_sample_1],\n",
        "                          [1, feature_1_sample_2, feature_2_sample_2],\n",
        "                          ...\n",
        "                          [1, feature_1_sample_n, feature_2_sample_n]]\n",
        "    - y (numpy.ndarray): Target values of shape (num_samples,).\n",
        "    - theta (numpy.ndarray): Initial parameter vector.\n",
        "    - learning_rate (float): Learning rate for stochastic gradient descent.\n",
        "    - momentum (float): Momentum coefficient.\n",
        "    - iterations (int): Number of iterations for stochastic gradient descent.\n",
        "\n",
        "    Returns:\n",
        "    - theta (numpy.ndarray): Optimized parameter vector.\n",
        "    - cost_history (numpy.ndarray): Array of cost values over iterations.\n",
        "    \"\"\"\n",
        "    num_samples = len(y)\n",
        "    cost_history = []\n",
        "    velocity = np.zeros_like(theta)\n",
        "\n",
        "    # Loop through each iteration\n",
        "    for iteration in range(iterations):\n",
        "        total_cost = 0.0\n",
        "\n",
        "        # Randomly select a sample's index\n",
        "        random_index = np.random.randint(0, num_samples)\n",
        "\n",
        "        # Get the feature matrix and target value for the selected sample\n",
        "        sample_X = X[random_index, :].reshape(1, X.shape[1])\n",
        "        sample_y = y[random_index].reshape(1, 1)\n",
        "\n",
        "        _, residuals = calculate_prediction_residuals(theta, sample_X, sample_y)\n",
        "\n",
        "        # Calculate the gradient for the current sample\n",
        "        gradient = calculate_gradient(sample_X, residuals)\n",
        "\n",
        "        # Update the velocity using momentum\n",
        "        velocity = momentum * velocity +  (1-momentum) * gradient\n",
        "\n",
        "        # Update the parameter vector using the updated velocity\n",
        "        theta -= learning_rate * velocity\n",
        "\n",
        "        cost = calculate_cost(theta, sample_X, sample_y)\n",
        "        cost_history.append(cost)\n",
        "\n",
        "    return theta, cost_history"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9E9Ij3pEwJpF"
      },
      "source": [
        "**TODO** Call your implementaiton of SGD with Momentum and plot the Cost vs iteration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WNntSXOl4CBJ"
      },
      "outputs": [],
      "source": [
        "theta = theta_guess.copy()\n",
        "momentum = 0.9\n",
        "\n",
        "\n",
        "print(\"Start:\")\n",
        "print(\n",
        "    \"Theta0:          {:0.3f},\\nTheta1:          {:0.3f}\".format(\n",
        "        theta[0][0], theta[1][0]\n",
        "    )\n",
        ")\n",
        "X_b = np.c_[np.ones((len(X), 1)), X]\n",
        "theta, cost_history = stochastic_gradient_descent_with_momentum(\n",
        "    X_b, y, theta, learning_rate, momentum, num_iterations\n",
        ")\n",
        "final_theta['SGDM']= theta\n",
        "\n",
        "print(\"End:\")\n",
        "print(\n",
        "    \"Theta0:          {:0.3f},\\nTheta1:          {:0.3f}\".format(\n",
        "        theta[0][0], theta[1][0]\n",
        "    )\n",
        ")\n",
        "print(\"Final cost/MSE:  {:0.3f}\".format(cost_history[-1]))\n",
        "final_cost['SGDM']= cost_history[-1]\n",
        "\n",
        "fig,ax = plt.subplots(figsize=(10,8))\n",
        "_=ax.plot(range(num_iterations),cost_history[:num_iterations],'b.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x6pQsDGvY9cv"
      },
      "source": [
        "# Mini Batch Stochastic Gradient Descent\n",
        "\n",
        "Mini-batch stochastic gradient descent (MBSGD) is a variation of stochastic gradient descent (SGD) where updates are made using small batches of data instead of single examples.\n",
        "\n",
        "This strikes a balance between the efficiency of using the entire dataset (batch gradient descent) and the randomness of SGD.\n",
        "\n",
        "MBSGD computes gradients and updates model parameters in each iteration using a randomly selected mini-batch of data.\n",
        "\n",
        "This approach can lead to faster convergence and better utilization of computational resources compared to traditional SGD or full-batch gradient descent.\n",
        "\n",
        "The size of the mini-batch is a tunable hyperparameter that influences the trade-off between convergence speed and noise in the updates.\n",
        "\n",
        "**TODO**: In the next cell implement MBSGD. You can use this signature:\n",
        "\n",
        "```python\n",
        "def minibatch_gradient_descent(X, y, theta, learning_rate=0.01, num_iterations=10, batch_size=20):\n",
        "    \"\"\"\n",
        "    Perform mini-batch gradient descent to optimize parameters for linear regression.\n",
        "\n",
        "    Parameters:\n",
        "    - X (numpy.ndarray): Feature matrix of shape (num_samples, num_features).\n",
        "                         Each row represents a sample, and each column represents a feature.\n",
        "                         The first column should be filled with ones for the intercept term.\n",
        "                         Example:\n",
        "                         [[1, feature_1_sample_1, feature_2_sample_1],\n",
        "                          [1, feature_1_sample_2, feature_2_sample_2],\n",
        "                          ...\n",
        "                          [1, feature_1_sample_n, feature_2_sample_n]]\n",
        "    - y (numpy.ndarray): Target values of shape (num_samples,).\n",
        "    - theta (numpy.ndarray): Initial parameter vector of shape (num_features,).\n",
        "    - learning_rate (float): Learning rate for gradient descent.\n",
        "    - num_iterations (int): Number of iterations for gradient descent.\n",
        "    - batch_size (int): Size of each mini-batch.\n",
        "\n",
        "    Returns:\n",
        "    - theta (numpy.ndarray): Optimized parameter vector.\n",
        "    - cost_history (list): List of cost values over iterations.\n",
        "    \"\"\"\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XQd6KV8oY9cw"
      },
      "outputs": [],
      "source": [
        "def minibatch_gradient_descent(X, y, theta, learning_rate=0.01, num_iterations=10, batch_size=20):\n",
        "    \"\"\"\n",
        "    Perform mini-batch gradient descent to optimize parameters for linear regression.\n",
        "\n",
        "    Parameters:\n",
        "    - X (numpy.ndarray): Feature matrix of shape (num_samples, num_features).\n",
        "                         Each row represents a sample, and each column represents a feature.\n",
        "                         The first column should be filled with ones for the intercept term.\n",
        "                         Example:\n",
        "                         [[1, feature_1_sample_1, feature_2_sample_1],\n",
        "                          [1, feature_1_sample_2, feature_2_sample_2],\n",
        "                          ...\n",
        "                          [1, feature_1_sample_n, feature_2_sample_n]]\n",
        "    - y (numpy.ndarray): Target values of shape (num_samples,).\n",
        "    - theta (numpy.ndarray): Initial parameter vector of shape (num_features,).\n",
        "    - learning_rate (float): Learning rate for gradient descent.\n",
        "    - num_iterations (int): Number of iterations for gradient descent.\n",
        "    - batch_size (int): Size of each mini-batch.\n",
        "\n",
        "    Returns:\n",
        "    - theta (numpy.ndarray): Optimized parameter vector.\n",
        "    - cost_history (list): List of cost values over iterations.\n",
        "    \"\"\"\n",
        "\n",
        "    # shape of X: (num_samples, num_features)\n",
        "    # shape of y: (num_samples,)\n",
        "    # shape of theta: (num_features,)\n",
        "\n",
        "    num_samples, num_features = X.shape\n",
        "\n",
        "    # Initialize cost history\n",
        "    cost_history = []\n",
        "\n",
        "    # Iterate over the number of iterations\n",
        "    for i in range(num_iterations):\n",
        "        # Randomly select a batch of size `batch_size` from the data\n",
        "        idx = np.random.choice(num_samples, batch_size)\n",
        "        X_batch, y_batch = X[idx], y[idx]\n",
        "\n",
        "        # Calculate the gradient for the current mini-batch\n",
        "        _, residuals = calculate_prediction_residuals(theta, X_batch, y_batch)\n",
        "\n",
        "        gradient = calculate_gradient(X_batch, residuals)\n",
        "\n",
        "        # Update the parameters\n",
        "        theta -= learning_rate * gradient\n",
        "\n",
        "        # Calculate the cost for the current iteration\n",
        "        cost = calculate_cost(theta, X_batch, y_batch)\n",
        "        cost_history.append(cost)\n",
        "\n",
        "    return theta, cost_history\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sCCLVucvxAn3"
      },
      "source": [
        "**TODO**: Run your implementation and plot the cost vs iteration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "erQifW90Y9cy"
      },
      "outputs": [],
      "source": [
        "theta = theta_guess.copy()\n",
        "batch_size = 10\n",
        "\n",
        "print(\"Start:\")\n",
        "print(\n",
        "    \"Theta0:          {:0.3f},\\nTheta1:          {:0.3f}\".format(\n",
        "        theta[0][0], theta[1][0]\n",
        "    )\n",
        ")\n",
        "\n",
        "# Add a column of ones to the feature matrix X:\n",
        "X_b = np.c_[np.ones((len(X), 1)), X]\n",
        "\n",
        "# Run Gradient Descent:\n",
        "theta, cost_history = minibatch_gradient_descent(\n",
        "    X_b, y, theta, learning_rate, num_iterations, batch_size = batch_size\n",
        ")\n",
        "final_theta['MBGD']= theta\n",
        "\n",
        "print(\"End:\")\n",
        "print(\n",
        "    \"Theta0:          {:0.3f},\\nTheta1:          {:0.3f}\".format(\n",
        "        theta[0][0], theta[1][0]\n",
        "    )\n",
        ")\n",
        "print(\"Final cost/MSE:  {:0.3f}\".format(cost_history[-1]))\n",
        "final_cost['MBGD']= cost_history[-1]\n",
        "fig,ax = plt.subplots(figsize=(10,8))\n",
        "\n",
        "ax.set_ylabel('{J(Theta)}',rotation=0)\n",
        "ax.set_xlabel('{Iterations}')\n",
        "theta = np.random.randn(2,1)\n",
        "\n",
        "ax.plot(range(num_iterations), cost_history,'b.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "szElqg9sTqmA"
      },
      "source": [
        "# (Optional) Print the final Cost and $\\theta$ for each algorithm."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ize5IfX6TqEw"
      },
      "outputs": [],
      "source": [
        "pp.pprint(final_cost)\n",
        "print()\n",
        "pp.pprint(final_theta)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zW-wH0WKhNRn"
      },
      "source": [
        "## TODO: Answer the following questions in the next cell.\n",
        "\n",
        "For these algorithms: GD, SGD, SGD with momentum, MBGD, answer these questions:\n",
        "\n",
        "1. Assuming the learning rate is the same for all algorithms, what is the worst case runtime complexity of each algorithm. Your answer should be a function of `num_samples`, `num_of_iteration`, and `batch_size`.\n",
        "1. How many times per iteration do we calculate the gradient in each?\n",
        "2. How would you compare the efficiency of these algorithms?\n",
        "3. How would you compare the quality of the result?\n",
        "4. Can you use the same learning rate for each of these algorithms?\n",
        "5. Assuming the same rating rate and number of iterations, how many operations does each method performs in total?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SD8Nm3eTxV3e"
      },
      "source": [
        "**TODO** Write your answers here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hwf91x4pxHDn"
      },
      "source": [
        "# Linear Regression with PyTorch\n",
        "\n",
        "Linear Regression can be implemented using PyTorch by defining a linear regression model, specifying a loss function (usually mean squared error), and using an optimization algorithm (e.g., stochastic gradient descent) to update the model parameters to minimize the loss.\n",
        "\n",
        "**TODO:** In the next cell write a function to perform linear regression using PyTorch.\n",
        "\n",
        "* Use this signature:\n",
        "\n",
        "```python\n",
        "def train_linear_regression(X, y, optimizer_type='SGD', learning_rate=0.01, num_epochs=1000):\n",
        "    \"\"\"\n",
        "    Train a linear regression model using PyTorch.\n",
        "\n",
        "    Parameters:\n",
        "    - X (torch.Tensor): Input data tensor of shape (num_samples, num_features).\n",
        "    - y (torch.Tensor): Target data tensor of shape (num_samples, num_targets).\n",
        "    - optimizer_type (str): Type of optimizer to use. Options: 'SGD', 'Adam', etc.\n",
        "    - learning_rate (float): Learning rate for the optimizer.\n",
        "    - num_epochs (int): Number of training epochs.\n",
        "\n",
        "    Returns:\n",
        "    - model (nn.Module): Trained linear regression model.\n",
        "    \"\"\"\n",
        "```    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5LWJN65sxJZn"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "def train_linear_regression(X, y, optimizer_type='SGD', learning_rate=0.01, num_epochs=1000):\n",
        "    \"\"\"\n",
        "    Train a linear regression model using PyTorch.\n",
        "\n",
        "    Parameters:\n",
        "    - X (torch.Tensor): Input data tensor of shape (num_samples, num_features).\n",
        "    - y (torch.Tensor): Target data tensor of shape (num_samples, num_targets).\n",
        "    - optimizer_type (str): Type of optimizer to use. Options: 'SGD', 'Adam', etc.\n",
        "    - learning_rate (float): Learning rate for the optimizer.\n",
        "    - num_epochs (int): Number of training epochs.\n",
        "\n",
        "    Returns:\n",
        "    - model (nn.Module): Trained linear regression model.\n",
        "    \"\"\"\n",
        "\n",
        "    return None\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZF-xF4MTxO7S"
      },
      "source": [
        "**TODO:**\n",
        "* Run your implementation `train_linear_regression.\n",
        "* Plot the trained line on top of the X,y plot.\n",
        "* Print your model parameters, i.e. $\\theta$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pFeBUXQ9xRy8"
      },
      "outputs": [],
      "source": [
        "X_tensor = torch.tensor(X, dtype=torch.float32)\n",
        "y_tensor = torch.tensor(y, dtype=torch.float32)\n",
        "trained_model = train_linear_regression(X_tensor, y_tensor, learning_rate=learning_rate, num_epochs=2000)\n",
        "\n",
        "# Print the model parameters\n",
        "weight, bias = trained_model.linear.weight[0].data[0], trained_model.linear.bias.data[0]\n",
        "dbg(weight, bias)\n",
        "\n",
        "learned_line = weight * X + bias\n",
        "\n",
        "\n",
        "# plt.plot(X,y,'b.')\n",
        "# plt.xlabel(\"$x$\", fontsize=18)\n",
        "# plt.ylabel(\"$y$\", rotation=0, fontsize=18)\n",
        "# plt.axis([0,np.max(X),0,np.max(y)])\n",
        "\n",
        "\n",
        "plt.scatter(X, y, color='blue', label='Data Points')\n",
        "plt.plot(X, learned_line, color='red', label='Learned Line')\n",
        "\n",
        "plt.xlabel('Input')\n",
        "plt.ylabel('Output')\n",
        "plt.legend()\n",
        "plt.title('Linear Regression')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "test save in colab"
      ],
      "metadata": {
        "id": "fM2LSMwZO-md"
      }
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "j0CVFwvtY9bW",
        "tuI2x9deY9br"
      ],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}